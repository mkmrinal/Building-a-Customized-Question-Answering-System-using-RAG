{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48148ddd-017d-41e6-bea9-128ee525c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Fetching data for Delhi, India...\n",
      "ðŸŒ Fetching Wikipedia summary for Delhi...\n",
      "ðŸ“ Fetching data for Mumbai, India...\n",
      "ðŸŒ Fetching Wikipedia summary for Mumbai...\n",
      "ðŸ“ Fetching data for Bangalore, India...\n",
      "ðŸŒ Fetching Wikipedia summary for Bangalore...\n",
      "ðŸ“ Fetching data for Kolkata, India...\n",
      "ðŸŒ Fetching Wikipedia summary for Kolkata...\n",
      "ðŸ“ Fetching data for Chennai, India...\n",
      "ðŸŒ Fetching Wikipedia summary for Chennai...\n",
      "ðŸ“ Fetching data for New York, USA...\n",
      "ðŸŒ Fetching Wikipedia summary for New York...\n",
      "ðŸ“ Fetching data for Los Angeles, USA...\n",
      "ðŸŒ Fetching Wikipedia summary for Los Angeles...\n",
      "ðŸ“ Fetching data for Chicago, USA...\n",
      "ðŸŒ Fetching Wikipedia summary for Chicago...\n",
      "ðŸ“ Fetching data for Houston, USA...\n",
      "ðŸŒ Fetching Wikipedia summary for Houston...\n",
      "ðŸ“ Fetching data for San Francisco, USA...\n",
      "ðŸŒ Fetching Wikipedia summary for San Francisco...\n",
      "ðŸ“ Fetching data for London, UK...\n",
      "ðŸŒ Fetching Wikipedia summary for London...\n",
      "ðŸ“ Fetching data for Manchester, UK...\n",
      "ðŸŒ Fetching Wikipedia summary for Manchester...\n",
      "ðŸ“ Fetching data for Birmingham, UK...\n",
      "ðŸŒ Fetching Wikipedia summary for Birmingham...\n",
      "ðŸ“ Fetching data for Glasgow, UK...\n",
      "ðŸŒ Fetching Wikipedia summary for Glasgow...\n",
      "ðŸ“ Fetching data for Liverpool, UK...\n",
      "ðŸŒ Fetching Wikipedia summary for Liverpool...\n",
      "ðŸ“ Fetching data for Berlin, Germany...\n",
      "ðŸŒ Fetching Wikipedia summary for Berlin...\n",
      "ðŸ“ Fetching data for Munich, Germany...\n",
      "ðŸŒ Fetching Wikipedia summary for Munich...\n",
      "ðŸ“ Fetching data for Hamburg, Germany...\n",
      "ðŸŒ Fetching Wikipedia summary for Hamburg...\n",
      "ðŸ“ Fetching data for Frankfurt, Germany...\n",
      "ðŸŒ Fetching Wikipedia summary for Frankfurt...\n",
      "ðŸ“ Fetching data for Cologne, Germany...\n",
      "ðŸŒ Fetching Wikipedia summary for Cologne...\n",
      "ðŸ“ Fetching data for Paris, France...\n",
      "ðŸŒ Fetching Wikipedia summary for Paris...\n",
      "ðŸ“ Fetching data for Marseille, France...\n",
      "ðŸŒ Fetching Wikipedia summary for Marseille...\n",
      "ðŸ“ Fetching data for Lyon, France...\n",
      "ðŸŒ Fetching Wikipedia summary for Lyon...\n",
      "ðŸ“ Fetching data for Toulouse, France...\n",
      "ðŸŒ Fetching Wikipedia summary for Toulouse...\n",
      "ðŸ“ Fetching data for Nice, France...\n",
      "ðŸŒ Fetching Wikipedia summary for Nice...\n",
      "ðŸ“ Fetching data for Tokyo, Japan...\n",
      "ðŸŒ Fetching Wikipedia summary for Tokyo...\n",
      "ðŸ“ Fetching data for Osaka, Japan...\n",
      "ðŸŒ Fetching Wikipedia summary for Osaka...\n",
      "ðŸ“ Fetching data for Yokohama, Japan...\n",
      "ðŸŒ Fetching Wikipedia summary for Yokohama...\n",
      "ðŸ“ Fetching data for Nagoya, Japan...\n",
      "ðŸŒ Fetching Wikipedia summary for Nagoya...\n",
      "ðŸ“ Fetching data for Sapporo, Japan...\n",
      "ðŸŒ Fetching Wikipedia summary for Sapporo...\n",
      "ðŸ“ Fetching data for Sydney, Australia...\n",
      "ðŸŒ Fetching Wikipedia summary for Sydney...\n",
      "ðŸ“ Fetching data for Melbourne, Australia...\n",
      "ðŸŒ Fetching Wikipedia summary for Melbourne...\n",
      "ðŸ“ Fetching data for Brisbane, Australia...\n",
      "ðŸŒ Fetching Wikipedia summary for Brisbane...\n",
      "ðŸ“ Fetching data for Perth, Australia...\n",
      "ðŸŒ Fetching Wikipedia summary for Perth...\n",
      "ðŸ“ Fetching data for Adelaide, Australia...\n",
      "ðŸŒ Fetching Wikipedia summary for Adelaide...\n",
      "ðŸ“ Fetching data for Toronto, Canada...\n",
      "ðŸŒ Fetching Wikipedia summary for Toronto...\n",
      "ðŸ“ Fetching data for Vancouver, Canada...\n",
      "ðŸŒ Fetching Wikipedia summary for Vancouver...\n",
      "ðŸ“ Fetching data for Montreal, Canada...\n",
      "ðŸŒ Fetching Wikipedia summary for Montreal...\n",
      "ðŸ“ Fetching data for Calgary, Canada...\n",
      "ðŸŒ Fetching Wikipedia summary for Calgary...\n",
      "ðŸ“ Fetching data for Ottawa, Canada...\n",
      "ðŸŒ Fetching Wikipedia summary for Ottawa...\n",
      "ðŸ“ Fetching data for SÃ£o Paulo, Brazil...\n",
      "ðŸŒ Fetching Wikipedia summary for SÃ£o Paulo...\n",
      "ðŸ“ Fetching data for Rio de Janeiro, Brazil...\n",
      "ðŸŒ Fetching Wikipedia summary for Rio de Janeiro...\n",
      "ðŸ“ Fetching data for BrasÃ­lia, Brazil...\n",
      "ðŸŒ Fetching Wikipedia summary for BrasÃ­lia...\n",
      "ðŸ“ Fetching data for Salvador, Brazil...\n",
      "ðŸŒ Fetching Wikipedia summary for Salvador...\n",
      "ðŸ“ Fetching data for Fortaleza, Brazil...\n",
      "ðŸŒ Fetching Wikipedia summary for Fortaleza...\n",
      "ðŸ“ Fetching data for Shanghai, China...\n",
      "ðŸŒ Fetching Wikipedia summary for Shanghai...\n",
      "ðŸ“ Fetching data for Beijing, China...\n",
      "ðŸŒ Fetching Wikipedia summary for Beijing...\n",
      "ðŸ“ Fetching data for Guangzhou, China...\n",
      "ðŸŒ Fetching Wikipedia summary for Guangzhou...\n",
      "ðŸ“ Fetching data for Shenzhen, China...\n",
      "ðŸŒ Fetching Wikipedia summary for Shenzhen...\n",
      "ðŸ“ Fetching data for Chengdu, China...\n",
      "ðŸŒ Fetching Wikipedia summary for Chengdu...\n",
      "âœ… Collected data for 50 major cities. Saved as 'major_cities.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import overpy\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Initialize Overpass API\n",
    "api = overpy.Overpass()\n",
    "\n",
    "# List of major cities by country\n",
    "major_cities = {\n",
    "    \"India\": [\"Delhi\", \"Mumbai\", \"Bangalore\", \"Kolkata\", \"Chennai\"],\n",
    "    \"USA\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"San Francisco\"],\n",
    "    \"UK\": [\"London\", \"Manchester\", \"Birmingham\", \"Glasgow\", \"Liverpool\"],\n",
    "    \"Germany\": [\"Berlin\", \"Munich\", \"Hamburg\", \"Frankfurt\", \"Cologne\"],\n",
    "    \"France\": [\"Paris\", \"Marseille\", \"Lyon\", \"Toulouse\", \"Nice\"],\n",
    "    \"Japan\": [\"Tokyo\", \"Osaka\", \"Yokohama\", \"Nagoya\", \"Sapporo\"],\n",
    "    \"Australia\": [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \"Adelaide\"],\n",
    "    \"Canada\": [\"Toronto\", \"Vancouver\", \"Montreal\", \"Calgary\", \"Ottawa\"],\n",
    "    \"Brazil\": [\"SÃ£o Paulo\", \"Rio de Janeiro\", \"BrasÃ­lia\", \"Salvador\", \"Fortaleza\"],\n",
    "    \"China\": [\"Shanghai\", \"Beijing\", \"Guangzhou\", \"Shenzhen\", \"Chengdu\"]\n",
    "}\n",
    "\n",
    "# Create folder for PDFs\n",
    "os.makedirs(\"city_data\", exist_ok=True)\n",
    "\n",
    "# Function to fetch city data from Overpass API\n",
    "def fetch_city_data(city_name, country_name):\n",
    "    print(f\"ðŸ“ Fetching data for {city_name}, {country_name}...\")\n",
    "\n",
    "    # First query: Search by city name and alternative names\n",
    "    query = f\"\"\"\n",
    "    [out:json][timeout:100];\n",
    "    (\n",
    "      node[\"name\"=\"{city_name}\"][\"place\"~\"city|town|metropolitan_area\"];\n",
    "      node[\"name:en\"=\"{city_name}\"][\"place\"~\"city|town|metropolitan_area\"];\n",
    "      node[\"alt_name\"=\"{city_name}\"][\"place\"~\"city|town|metropolitan_area\"];\n",
    "    );\n",
    "    out body;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = api.query(query)\n",
    "    except overpy.exception.OverpassRuntimeError as e:\n",
    "        print(f\"âš ï¸ Overpass API error for {city_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # If no results, fallback to a broad location-based search\n",
    "    if not result.nodes:\n",
    "        print(f\"âš ï¸ No exact match for {city_name}. Trying bounding box search...\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:100];\n",
    "        (\n",
    "          node[\"place\"~\"city|town|metropolitan_area\"](around:500000, 28.6448, 77.216721);\n",
    "        );\n",
    "        out body;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = api.query(query)\n",
    "        except overpy.exception.OverpassRuntimeError as e:\n",
    "            print(f\"âš ï¸ Overpass API error (fallback) for {city_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if not result.nodes:\n",
    "            print(f\"âš ï¸ No data found for {city_name}. Skipping...\")\n",
    "            return None\n",
    "\n",
    "    node = result.nodes[0]\n",
    "    return {\n",
    "        \"city\": node.tags.get(\"name\", \"Unknown\"),\n",
    "        \"state\": node.tags.get(\"is_in:state\", \"N/A\"),\n",
    "        \"population\": int(node.tags.get(\"population\", 0)) if node.tags.get(\"population\", \"0\").isdigit() else \"N/A\",\n",
    "        \"latitude\": float(node.lat),\n",
    "        \"longitude\": float(node.lon),\n",
    "        \"altitude\": node.tags.get(\"ele\", \"N/A\"),\n",
    "        \"timezone\": node.tags.get(\"timezone\", \"N/A\"),\n",
    "        \"landmarks\": \"\",\n",
    "        \"climate\": \"\"\n",
    "    }\n",
    "\n",
    "# Function to get Wikipedia summary for a city\n",
    "def get_wikipedia_summary(city_name):\n",
    "    print(f\"ðŸŒ Fetching Wikipedia summary for {city_name}...\")\n",
    "\n",
    "    search_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{city_name.replace(' ', '_')}\"\n",
    "    try:\n",
    "        response = requests.get(search_url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return \"Wikipedia page not found\"\n",
    "\n",
    "        data = response.json()\n",
    "        return data.get(\"extract\", \"No relevant information found\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching Wikipedia: {e}\"\n",
    "\n",
    "# Collect data for all major cities\n",
    "all_cities = []\n",
    "for country, cities in major_cities.items():\n",
    "    for city in cities:\n",
    "        city_data = fetch_city_data(city, country)\n",
    "        if city_data:\n",
    "            city_data[\"country\"] = country\n",
    "            city_data[\"landmarks\"] = get_wikipedia_summary(city)  # Get Wikipedia info\n",
    "            all_cities.append(city_data)\n",
    "        time.sleep(2)  # Prevent API rate limits\n",
    "\n",
    "# Save collected data as JSON\n",
    "with open(\"major_cities.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_cities, f, indent=4)\n",
    "\n",
    "print(f\"âœ… Collected data for {len(all_cities)} major cities. Saved as 'major_cities.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58967fd9-2a73-4c22-9c78-67b0936639b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mmukh\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "Question: Which city has the highest population in India?\n",
      "â—‹ Sub-question responses:\n",
      "â–  \"Mumbai: 12.4M\"\n",
      "â–  \"Bengaluru: 10.8M\"\n",
      "â–  \"Chennai: 4.7M\"\n",
      "â—‹ Final Response: \"Mumbai has the highest population in India at 12.4M.\"\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configuration\n",
    "GEMINI_API_KEY = \"AIzaSyBGg1NhYaZ3uXk-b96cUq4WQW_LcLq5Hsk\"\n",
    "GEMINI_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key={GEMINI_API_KEY}\"\n",
    "\n",
    "# Load city data\n",
    "with open(\"major_cities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    city_data = json.load(f)\n",
    "\n",
    "# Initialize embedding model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Prepare embeddings for vector retrieval\n",
    "texts = []\n",
    "for city in city_data:\n",
    "    text = f\"\"\"\n",
    "    City: {city['city']}\n",
    "    Country: {city['country']}\n",
    "    Population: {city['population']}\n",
    "    Altitude: {city['altitude']}\n",
    "    Landmarks: {city['landmarks']}\n",
    "    Climate: {city['climate']}\n",
    "    \"\"\"\n",
    "    texts.append(text)\n",
    "\n",
    "embeddings = model.encode(texts).astype(np.float32)\n",
    "embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Create FAISS index for vector retrieval\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "def generate_sub_questions(query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Task 1: Sub-question Generation\n",
    "    Decomposes complex questions into sub-questions with source mapping and retrieval methods\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this question and generate 2-3 sub-questions needed to answer it.\n",
    "    For each, specify:\n",
    "    1. The sub-question text\n",
    "    2. Required data attributes\n",
    "    3. Retrieval method (vector/summary)\n",
    "    4. Data source ('city_dataset' for our data)\n",
    "    \n",
    "    Use this exact JSON format:\n",
    "    {{\n",
    "        \"question\": \"sub-question text\",\n",
    "        \"attributes\": [\"list\", \"of\", \"attributes\"],\n",
    "        \"retrieval\": \"vector|summary\",\n",
    "        \"source\": \"city_dataset\"\n",
    "    }}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Return only a valid JSON list of these objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "        \"generationConfig\": {\"temperature\": 0.1}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(GEMINI_URL, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return json.loads(response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: Simple question with all attributes and vector retrieval\n",
    "    return [{\n",
    "        \"question\": query,\n",
    "        \"attributes\": [\"population\", \"altitude\", \"landmarks\", \"climate\"],\n",
    "        \"retrieval\": \"vector\",\n",
    "        \"source\": \"city_dataset\"\n",
    "    }]\n",
    "\n",
    "def vector_retrieval(query: str, attributes: List[str], top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Vector retrieval using FAISS index\"\"\"\n",
    "    query_embedding = model.encode([query]).astype(np.float32)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)\n",
    "    \n",
    "    scores, indices = index.search(query_embedding, top_k * 3)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        city = city_data[idx]\n",
    "        result = {\"city\": city[\"city\"], \"country\": city[\"country\"], \"score\": float(score)}\n",
    "        for attr in attributes:\n",
    "            if attr in city:\n",
    "                result[attr] = city[attr]\n",
    "        results.append(result)\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)[:top_k]\n",
    "\n",
    "def summary_retrieval(query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Summary retrieval returns full city records\"\"\"\n",
    "    return city_data[:3]  # Return first 3 cities as fallback\n",
    "\n",
    "def retrieve_city_data(sub_question: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Task 2: Vector/Summary Retrieval\n",
    "    Handles both retrieval methods based on sub-question specification\n",
    "    \"\"\"\n",
    "    if sub_question[\"retrieval\"] == \"vector\":\n",
    "        return vector_retrieval(sub_question[\"question\"], sub_question[\"attributes\"])\n",
    "    else:\n",
    "        return summary_retrieval(sub_question[\"question\"])\n",
    "\n",
    "def format_sub_answer(city: Dict[str, Any], attribute: str) -> str:\n",
    "    \"\"\"Formats individual city data for sub-question responses\"\"\"\n",
    "    if attribute == \"population\":\n",
    "        if not isinstance(city.get('population'), (int, float)) or city['population'] <= 0:\n",
    "            return None\n",
    "        return f\"{city['city']}: {round(city['population']/1000000, 1)}M\"\n",
    "    elif attribute == \"altitude\":\n",
    "        if not city.get('altitude') or city['altitude'] == \"N/A\":\n",
    "            return None\n",
    "        return f\"{city['city']}: {city['altitude']}m\"\n",
    "    elif attribute == \"landmarks\":\n",
    "        if not city.get('landmarks'):\n",
    "            return None\n",
    "        return f\"{city['city']}: {city['landmarks'].split('.')[0].strip()}\"\n",
    "    elif attribute == \"climate\":\n",
    "        if not city.get('climate'):\n",
    "            return None\n",
    "        return f\"{city['city']}: {city['climate'].split('.')[0].strip()}\"\n",
    "    return f\"{city['city']}, {city['country']}\"\n",
    "\n",
    "def generate_final_response(query: str, sub_answers: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Task 3: Response Aggregation\n",
    "    Properly handles all question types and data fields from the city data\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    final_answer = \"Couldn't find specific information to answer this question.\"\n",
    "    sub_responses = []\n",
    "    \n",
    "    # Extract country from query if specified\n",
    "    query_lower = query.lower()\n",
    "    target_country = None\n",
    "    for country in ['india', 'usa', 'china', 'japan', 'uk', 'germany', 'france', 'australia', 'canada', 'brazil']:\n",
    "        if country in query_lower:\n",
    "            target_country = country\n",
    "            break\n",
    "    \n",
    "    # Extract target city if specified\n",
    "    target_city = None\n",
    "    for city in [c['city'].lower() for c in city_data]:\n",
    "        if city in query_lower:\n",
    "            target_city = city\n",
    "            break\n",
    "    \n",
    "    # Prepare data containers\n",
    "    altitude_data = []\n",
    "    population_data = []\n",
    "    landmark_data = []\n",
    "    climate_data = []\n",
    "    \n",
    "    # Collect all relevant city data\n",
    "    for city in city_data:\n",
    "        # Skip if country filter exists and doesn't match\n",
    "        if target_country and city['country'].lower() != target_country:\n",
    "            continue\n",
    "        \n",
    "        # Skip if city filter exists and doesn't match\n",
    "        if target_city and city['city'].lower() != target_city:\n",
    "            continue\n",
    "        \n",
    "        # Process altitude data\n",
    "        if city.get('altitude') and city['altitude'] != 'N/A':\n",
    "            try:\n",
    "                altitude = float(city['altitude'].replace('m', '')) if 'm' in city['altitude'] else float(city['altitude'])\n",
    "                altitude_data.append({\n",
    "                    'city': city['city'],\n",
    "                    'value': f\"{altitude}m\",\n",
    "                    'numeric': altitude\n",
    "                })\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Process population data\n",
    "        if city.get('population') and isinstance(city['population'], (int, float)):\n",
    "            population_data.append({\n",
    "                'city': city['city'],\n",
    "                'value': f\"{round(city['population']/1000000, 1)}M\",\n",
    "                'numeric': city['population']\n",
    "            })\n",
    "        \n",
    "        # Process landmark data\n",
    "        if city.get('landmarks') and city['landmarks'] not in ['N/A', '']:\n",
    "            landmark_data.append({\n",
    "                'city': city['city'],\n",
    "                'value': city['landmarks'].split('.')[0].strip()\n",
    "            })\n",
    "        \n",
    "        # Process climate data\n",
    "        if city.get('climate') and city['climate'] not in ['N/A', '']:\n",
    "            climate_data.append({\n",
    "                'city': city['city'],\n",
    "                'value': city['climate'].split('.')[0].strip()\n",
    "            })\n",
    "\n",
    "    # Determine question type and format response\n",
    "    # Altitude questions\n",
    "    if any(keyword in query_lower for keyword in ['altitude', 'elevation', 'height']):\n",
    "        if altitude_data:\n",
    "            altitude_data.sort(key=lambda x: x['numeric'], reverse='highest' in query_lower)\n",
    "            top_entry = altitude_data[0]\n",
    "            \n",
    "            if 'highest' in query_lower:\n",
    "                final_answer = f\"{top_entry['city']} has the highest altitude in {target_country.title() if target_country else 'the world'} at {top_entry['value']}.\"\n",
    "            elif 'lowest' in query_lower:\n",
    "                final_answer = f\"{top_entry['city']} has the lowest altitude in {target_country.title() if target_country else 'the world'} at {top_entry['value']}.\"\n",
    "            else:\n",
    "                final_answer = f\"{top_entry['city']} has altitude {top_entry['value']}.\"\n",
    "            \n",
    "            sub_responses = [f\"{x['city']}: {x['value']}\" for x in altitude_data[:3]]\n",
    "    \n",
    "    # Population questions\n",
    "    elif any(keyword in query_lower for keyword in ['population', 'populous']):\n",
    "        if population_data:\n",
    "            population_data.sort(key=lambda x: x['numeric'], reverse=True)\n",
    "            top_entry = population_data[0]\n",
    "            \n",
    "            if 'highest' in query_lower:\n",
    "                final_answer = f\"{top_entry['city']} has the highest population in {target_country.title() if target_country else 'the world'} at {top_entry['value']}.\"\n",
    "            elif 'lowest' in query_lower:\n",
    "                final_answer = f\"{top_entry['city']} has the lowest population in {target_country.title() if target_country else 'the world'} at {top_entry['value']}.\"\n",
    "            else:\n",
    "                final_answer = f\"{top_entry['city']} has population {top_entry['value']}.\"\n",
    "            \n",
    "            sub_responses = [f\"{x['city']}: {x['value']}\" for x in population_data[:3]]\n",
    "    \n",
    "    # Landmark questions\n",
    "    elif any(keyword in query_lower for keyword in ['landmark', 'place', 'attraction', 'tourist', 'visit', 'see']):\n",
    "        if landmark_data:\n",
    "            if target_city:\n",
    "                landmarks = [x['value'] for x in landmark_data if x['city'].lower() == target_city]\n",
    "                final_answer = f\"Places to visit in {target_city.title()}: {', '.join(landmarks[:3])}.\"\n",
    "            else:\n",
    "                landmarks = [f\"{x['city']}: {x['value']}\" for x in landmark_data]\n",
    "                final_answer = f\"Top landmarks: {', '.join(landmarks[:3])}.\"\n",
    "            \n",
    "            sub_responses = [f\"{x['city']}: {x['value']}\" for x in landmark_data[:3]]\n",
    "    \n",
    "    # Climate questions\n",
    "    elif any(keyword in query_lower for keyword in ['climate', 'weather', 'temperature']):\n",
    "        if climate_data:\n",
    "            if target_city:\n",
    "                climates = [x['value'] for x in climate_data if x['city'].lower() == target_city]\n",
    "                final_answer = f\"Climate in {target_city.title()}: {', '.join(climates[:3])}.\"\n",
    "            else:\n",
    "                climates = [f\"{x['city']}: {x['value']}\" for x in climate_data]\n",
    "                final_answer = f\"Climate information: {', '.join(climates[:3])}.\"\n",
    "            \n",
    "            sub_responses = [f\"{x['city']}: {x['value']}\" for x in climate_data[:3]]\n",
    "    \n",
    "    # Format the output\n",
    "    response_lines = [\"â—‹ Sub-question responses:\"]\n",
    "    for i, resp in enumerate(sub_responses[:3], 1):\n",
    "        response_lines.append(f'â–  \"{resp}\"')\n",
    "    response_lines.append(f'â—‹ Final Response: \"{final_answer}\"')\n",
    "    \n",
    "    return '\\n'.join(response_lines)\n",
    "\n",
    "def answer_city_question(query: str) -> str:\n",
    "    \"\"\"Complete RAG pipeline execution\"\"\"\n",
    "    # Task 1: Generate sub-questions\n",
    "    sub_questions = generate_sub_questions(query)\n",
    "    \n",
    "    # Task 2: Retrieve answers for each sub-question\n",
    "    sub_answers = []\n",
    "    for sub_q in sub_questions:\n",
    "        cities = retrieve_city_data(sub_q)\n",
    "        answers = []\n",
    "        for city in cities:\n",
    "            # Try all requested attributes until we get a valid answer\n",
    "            for attr in sub_q[\"attributes\"]:\n",
    "                ans = format_sub_answer(city, attr)\n",
    "                if ans:\n",
    "                    answers.append(ans)\n",
    "                    break\n",
    "        \n",
    "        sub_answers.append({\n",
    "            \"question\": sub_q[\"question\"],\n",
    "            \"answers\": answers,\n",
    "            \"source\": sub_q.get(\"source\", \"city_dataset\")\n",
    "        })\n",
    "    \n",
    "    # Task 3: Generate final response\n",
    "    return generate_final_response(query, sub_answers)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_queries = [\n",
    "        \"Which city has the highest population in India?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuestion: {query}\")\n",
    "        print(answer_city_question(query))\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6301c84-d61d-46db-92e0-baaf04e6d97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
