{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48148ddd-017d-41e6-bea9-128ee525c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍 Fetching data for Delhi, India...\n",
      "🌍 Fetching Wikipedia summary for Delhi...\n",
      "📍 Fetching data for Mumbai, India...\n",
      "🌍 Fetching Wikipedia summary for Mumbai...\n",
      "📍 Fetching data for Bangalore, India...\n",
      "🌍 Fetching Wikipedia summary for Bangalore...\n",
      "📍 Fetching data for Kolkata, India...\n",
      "🌍 Fetching Wikipedia summary for Kolkata...\n",
      "📍 Fetching data for Chennai, India...\n",
      "🌍 Fetching Wikipedia summary for Chennai...\n",
      "📍 Fetching data for New York, USA...\n",
      "🌍 Fetching Wikipedia summary for New York...\n",
      "📍 Fetching data for Los Angeles, USA...\n",
      "🌍 Fetching Wikipedia summary for Los Angeles...\n",
      "📍 Fetching data for Chicago, USA...\n",
      "🌍 Fetching Wikipedia summary for Chicago...\n",
      "📍 Fetching data for Houston, USA...\n",
      "🌍 Fetching Wikipedia summary for Houston...\n",
      "📍 Fetching data for San Francisco, USA...\n",
      "🌍 Fetching Wikipedia summary for San Francisco...\n",
      "📍 Fetching data for London, UK...\n",
      "🌍 Fetching Wikipedia summary for London...\n",
      "📍 Fetching data for Manchester, UK...\n",
      "🌍 Fetching Wikipedia summary for Manchester...\n",
      "📍 Fetching data for Birmingham, UK...\n",
      "🌍 Fetching Wikipedia summary for Birmingham...\n",
      "📍 Fetching data for Glasgow, UK...\n",
      "🌍 Fetching Wikipedia summary for Glasgow...\n",
      "📍 Fetching data for Liverpool, UK...\n",
      "🌍 Fetching Wikipedia summary for Liverpool...\n",
      "📍 Fetching data for Berlin, Germany...\n",
      "🌍 Fetching Wikipedia summary for Berlin...\n",
      "📍 Fetching data for Munich, Germany...\n",
      "🌍 Fetching Wikipedia summary for Munich...\n",
      "📍 Fetching data for Hamburg, Germany...\n",
      "🌍 Fetching Wikipedia summary for Hamburg...\n",
      "📍 Fetching data for Frankfurt, Germany...\n",
      "🌍 Fetching Wikipedia summary for Frankfurt...\n",
      "📍 Fetching data for Cologne, Germany...\n",
      "🌍 Fetching Wikipedia summary for Cologne...\n",
      "📍 Fetching data for Paris, France...\n",
      "🌍 Fetching Wikipedia summary for Paris...\n",
      "📍 Fetching data for Marseille, France...\n",
      "🌍 Fetching Wikipedia summary for Marseille...\n",
      "📍 Fetching data for Lyon, France...\n",
      "🌍 Fetching Wikipedia summary for Lyon...\n",
      "📍 Fetching data for Toulouse, France...\n",
      "🌍 Fetching Wikipedia summary for Toulouse...\n",
      "📍 Fetching data for Nice, France...\n",
      "🌍 Fetching Wikipedia summary for Nice...\n",
      "📍 Fetching data for Tokyo, Japan...\n",
      "🌍 Fetching Wikipedia summary for Tokyo...\n",
      "📍 Fetching data for Osaka, Japan...\n",
      "🌍 Fetching Wikipedia summary for Osaka...\n",
      "📍 Fetching data for Yokohama, Japan...\n",
      "🌍 Fetching Wikipedia summary for Yokohama...\n",
      "📍 Fetching data for Nagoya, Japan...\n",
      "🌍 Fetching Wikipedia summary for Nagoya...\n",
      "📍 Fetching data for Sapporo, Japan...\n",
      "🌍 Fetching Wikipedia summary for Sapporo...\n",
      "📍 Fetching data for Sydney, Australia...\n",
      "🌍 Fetching Wikipedia summary for Sydney...\n",
      "📍 Fetching data for Melbourne, Australia...\n",
      "🌍 Fetching Wikipedia summary for Melbourne...\n",
      "📍 Fetching data for Brisbane, Australia...\n",
      "🌍 Fetching Wikipedia summary for Brisbane...\n",
      "📍 Fetching data for Perth, Australia...\n",
      "🌍 Fetching Wikipedia summary for Perth...\n",
      "📍 Fetching data for Adelaide, Australia...\n",
      "🌍 Fetching Wikipedia summary for Adelaide...\n",
      "📍 Fetching data for Toronto, Canada...\n",
      "🌍 Fetching Wikipedia summary for Toronto...\n",
      "📍 Fetching data for Vancouver, Canada...\n",
      "🌍 Fetching Wikipedia summary for Vancouver...\n",
      "📍 Fetching data for Montreal, Canada...\n",
      "🌍 Fetching Wikipedia summary for Montreal...\n",
      "📍 Fetching data for Calgary, Canada...\n",
      "🌍 Fetching Wikipedia summary for Calgary...\n",
      "📍 Fetching data for Ottawa, Canada...\n",
      "🌍 Fetching Wikipedia summary for Ottawa...\n",
      "📍 Fetching data for São Paulo, Brazil...\n",
      "🌍 Fetching Wikipedia summary for São Paulo...\n",
      "📍 Fetching data for Rio de Janeiro, Brazil...\n",
      "🌍 Fetching Wikipedia summary for Rio de Janeiro...\n",
      "📍 Fetching data for Brasília, Brazil...\n",
      "🌍 Fetching Wikipedia summary for Brasília...\n",
      "📍 Fetching data for Salvador, Brazil...\n",
      "🌍 Fetching Wikipedia summary for Salvador...\n",
      "📍 Fetching data for Fortaleza, Brazil...\n",
      "🌍 Fetching Wikipedia summary for Fortaleza...\n",
      "📍 Fetching data for Shanghai, China...\n",
      "🌍 Fetching Wikipedia summary for Shanghai...\n",
      "📍 Fetching data for Beijing, China...\n",
      "🌍 Fetching Wikipedia summary for Beijing...\n",
      "📍 Fetching data for Guangzhou, China...\n",
      "🌍 Fetching Wikipedia summary for Guangzhou...\n",
      "📍 Fetching data for Shenzhen, China...\n",
      "🌍 Fetching Wikipedia summary for Shenzhen...\n",
      "📍 Fetching data for Chengdu, China...\n",
      "🌍 Fetching Wikipedia summary for Chengdu...\n",
      "✅ Collected data for 50 major cities. Saved as 'major_cities.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import overpy\n",
    "from fpdf import FPDF\n",
    "\n",
    "# Initialize Overpass API\n",
    "api = overpy.Overpass()\n",
    "\n",
    "# List of major cities by country\n",
    "major_cities = {\n",
    "    \"India\": [\"Delhi\", \"Mumbai\", \"Bangalore\", \"Kolkata\", \"Chennai\"],\n",
    "    \"USA\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"San Francisco\"],\n",
    "    \"UK\": [\"London\", \"Manchester\", \"Birmingham\", \"Glasgow\", \"Liverpool\"],\n",
    "    \"Germany\": [\"Berlin\", \"Munich\", \"Hamburg\", \"Frankfurt\", \"Cologne\"],\n",
    "    \"France\": [\"Paris\", \"Marseille\", \"Lyon\", \"Toulouse\", \"Nice\"],\n",
    "    \"Japan\": [\"Tokyo\", \"Osaka\", \"Yokohama\", \"Nagoya\", \"Sapporo\"],\n",
    "    \"Australia\": [\"Sydney\", \"Melbourne\", \"Brisbane\", \"Perth\", \"Adelaide\"],\n",
    "    \"Canada\": [\"Toronto\", \"Vancouver\", \"Montreal\", \"Calgary\", \"Ottawa\"],\n",
    "    \"Brazil\": [\"São Paulo\", \"Rio de Janeiro\", \"Brasília\", \"Salvador\", \"Fortaleza\"],\n",
    "    \"China\": [\"Shanghai\", \"Beijing\", \"Guangzhou\", \"Shenzhen\", \"Chengdu\"]\n",
    "}\n",
    "\n",
    "# Create folder for PDFs\n",
    "os.makedirs(\"city_data\", exist_ok=True)\n",
    "\n",
    "# Function to fetch city data from Overpass API\n",
    "def fetch_city_data(city_name, country_name):\n",
    "    print(f\"📍 Fetching data for {city_name}, {country_name}...\")\n",
    "\n",
    "    # First query: Search by city name and alternative names\n",
    "    query = f\"\"\"\n",
    "    [out:json][timeout:100];\n",
    "    (\n",
    "      node[\"name\"=\"{city_name}\"][\"place\"~\"city|town|metropolitan_area\"];\n",
    "      node[\"name:en\"=\"{city_name}\"][\"place\"~\"city|town|metropolitan_area\"];\n",
    "      node[\"alt_name\"=\"{city_name}\"][\"place\"~\"city|town|metropolitan_area\"];\n",
    "    );\n",
    "    out body;\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        result = api.query(query)\n",
    "    except overpy.exception.OverpassRuntimeError as e:\n",
    "        print(f\"⚠️ Overpass API error for {city_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # If no results, fallback to a broad location-based search\n",
    "    if not result.nodes:\n",
    "        print(f\"⚠️ No exact match for {city_name}. Trying bounding box search...\")\n",
    "\n",
    "        query = f\"\"\"\n",
    "        [out:json][timeout:100];\n",
    "        (\n",
    "          node[\"place\"~\"city|town|metropolitan_area\"](around:500000, 28.6448, 77.216721);\n",
    "        );\n",
    "        out body;\n",
    "        \"\"\"\n",
    "        try:\n",
    "            result = api.query(query)\n",
    "        except overpy.exception.OverpassRuntimeError as e:\n",
    "            print(f\"⚠️ Overpass API error (fallback) for {city_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if not result.nodes:\n",
    "            print(f\"⚠️ No data found for {city_name}. Skipping...\")\n",
    "            return None\n",
    "\n",
    "    node = result.nodes[0]\n",
    "    return {\n",
    "        \"city\": node.tags.get(\"name\", \"Unknown\"),\n",
    "        \"state\": node.tags.get(\"is_in:state\", \"N/A\"),\n",
    "        \"population\": int(node.tags.get(\"population\", 0)) if node.tags.get(\"population\", \"0\").isdigit() else \"N/A\",\n",
    "        \"latitude\": float(node.lat),\n",
    "        \"longitude\": float(node.lon),\n",
    "        \"altitude\": node.tags.get(\"ele\", \"N/A\"),\n",
    "        \"timezone\": node.tags.get(\"timezone\", \"N/A\"),\n",
    "        \"landmarks\": \"\",\n",
    "        \"climate\": \"\"\n",
    "    }\n",
    "\n",
    "# Function to get Wikipedia summary for a city\n",
    "def get_wikipedia_summary(city_name):\n",
    "    print(f\"🌍 Fetching Wikipedia summary for {city_name}...\")\n",
    "\n",
    "    search_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{city_name.replace(' ', '_')}\"\n",
    "    try:\n",
    "        response = requests.get(search_url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return \"Wikipedia page not found\"\n",
    "\n",
    "        data = response.json()\n",
    "        return data.get(\"extract\", \"No relevant information found\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching Wikipedia: {e}\"\n",
    "\n",
    "# Collect data for all major cities\n",
    "all_cities = []\n",
    "for country, cities in major_cities.items():\n",
    "    for city in cities:\n",
    "        city_data = fetch_city_data(city, country)\n",
    "        if city_data:\n",
    "            city_data[\"country\"] = country\n",
    "            city_data[\"landmarks\"] = get_wikipedia_summary(city)  # Get Wikipedia info\n",
    "            all_cities.append(city_data)\n",
    "        time.sleep(2)  # Prevent API rate limits\n",
    "\n",
    "# Save collected data as JSON\n",
    "with open(\"major_cities.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_cities, f, indent=4)\n",
    "\n",
    "print(f\"✅ Collected data for {len(all_cities)} major cities. Saved as 'major_cities.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58967fd9-2a73-4c22-9c78-67b0936639b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mmukh\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "Question: Which city has the highest population in India?\n",
      "○ Sub-question responses:\n",
      "■ \"Mumbai: 12.4M\"\n",
      "■ \"Bengaluru: 10.8M\"\n",
      "■ \"Chennai: 4.7M\"\n",
      "○ Final Response: \"Mumbai has the highest population in India at 12.4M.\"\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configuration\n",
    "GEMINI_API_KEY = \"AIzaSyBGg1NhYaZ3uXk-b96cUq4WQW_LcLq5Hsk\"\n",
    "GEMINI_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key={GEMINI_API_KEY}\"\n",
    "\n",
    "# Load city data\n",
    "with open(\"major_cities.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    city_data = json.load(f)\n",
    "\n",
    "# Initialize embedding model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Prepare embeddings for vector retrieval\n",
    "texts = []\n",
    "for city in city_data:\n",
    "    text = f\"\"\"\n",
    "    City: {city['city']}\n",
    "    Country: {city['country']}\n",
    "    Population: {city['population']}\n",
    "    Altitude: {city['altitude']}\n",
    "    Landmarks: {city['landmarks']}\n",
    "    Climate: {city['climate']}\n",
    "    \"\"\"\n",
    "    texts.append(text)\n",
    "\n",
    "embeddings = model.encode(texts).astype(np.float32)\n",
    "embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Create FAISS index for vector retrieval\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "def generate_sub_questions(query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Task 1: Sub-question Generation\n",
    "    Decomposes complex questions into sub-questions with source mapping and retrieval methods\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this question and generate 2-3 sub-questions needed to answer it.\n",
    "    For each, specify:\n",
    "    1. The sub-question text\n",
    "    2. Required data attributes\n",
    "    3. Retrieval method (vector/summary)\n",
    "    4. Data source ('city_dataset' for our data)\n",
    "    \n",
    "    Use this exact JSON format:\n",
    "    {{\n",
    "        \"question\": \"sub-question text\",\n",
    "        \"attributes\": [\"list\", \"of\", \"attributes\"],\n",
    "        \"retrieval\": \"vector|summary\",\n",
    "        \"source\": \"city_dataset\"\n",
    "    }}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Return only a valid JSON list of these objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = {\n",
    "        \"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt}]}],\n",
    "        \"generationConfig\": {\"temperature\": 0.1}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(GEMINI_URL, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return json.loads(response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: Simple question with all attributes and vector retrieval\n",
    "    return [{\n",
    "        \"question\": query,\n",
    "        \"attributes\": [\"population\", \"altitude\", \"landmarks\", \"climate\"],\n",
    "        \"retrieval\": \"vector\",\n",
    "        \"source\": \"city_dataset\"\n",
    "    }]\n",
    "\n",
    "def vector_retrieval(query: str, attributes: List[str], top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Vector retrieval using FAISS index\"\"\"\n",
    "    query_embedding = model.encode([query]).astype(np.float32)\n",
    "    query_embedding /= np.linalg.norm(query_embedding)\n",
    "    \n",
    "    scores, indices = index.search(query_embedding, top_k * 3)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        city = city_data[idx]\n",
    "        result = {\"city\": city[\"city\"], \"country\": city[\"country\"], \"score\": float(score)}\n",
    "        for attr in attributes:\n",
    "            if attr in city:\n",
    "                result[attr] = city[attr]\n",
    "        results.append(result)\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)[:top_k]\n",
    "\n",
    "def summary_retrieval(query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Summary retrieval returns full city records\"\"\"\n",
    "    return city_data[:3]  # Return first 3 cities as fallback\n",
    "\n",
    "def retrieve_city_data(sub_question: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Task 2: Vector/Summary Retrieval\n",
    "    Handles both retrieval methods based on sub-question specification\n",
    "    \"\"\"\n",
    "    if sub_question[\"retrieval\"] == \"vector\":\n",
    "        return vector_retrieval(sub_question[\"question\"], sub_question[\"attributes\"])\n",
    "    else:\n",
    "        return summary_retrieval(sub_question[\"question\"])\n",
    "\n",
    "def format_sub_answer(city: Dict[str, Any], attribute: str) -> str:\n",
    "    \"\"\"Formats individual city data for sub-question responses\"\"\"\n",
    "    if attribute == \"population\":\n",
    "        if not isinstance(city.get('population'), (int, float)) or city['population'] <= 0:\n",
    "            return None\n",
    "        return f\"{city['city']}: {round(city['population']/1000000, 1)}M\"\n",
    "    elif attribute == \"altitude\":\n",
    "        if not city.get('altitude') or city['altitude'] == \"N/A\":\n",
    "            return None\n",
    "        return f\"{city['city']}: {city['altitude']}m\"\n",
    "    elif attribute == \"landmarks\":\n",
    "        if not city.get('landmarks'):\n",
    "            return None\n",
    "        return f\"{city['city']}: {city['landmarks'].split('.')[0].strip()}\"\n",
    "    elif attribute == \"climate\":\n",
    "        if not city.get('climate'):\n",
    "            return None\n",
    "        return f\"{city['city']}: {city['climate'].split('.')[0].strip()}\"\n",
    "    return f\"{city['city']}, {city['country']}\"\n",
    "\n",
    "def generate_final_response(query: str, sub_answers: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Task 3: Response Aggregation\n",
    "    Properly handles all question types and data fields from the city data\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    final_answer = \"Couldn't find specific information to answer this question.\"\n",
    "    sub_responses = []\n",
    "    \n",
    "    # Extract country from query if specified\n",
    "    query_lower = query.lower()\n",
    "    target_country = None\n",
    "    for country in ['india', 'usa', 'china', 'japan', 'uk', 'germany', 'france', 'australia', 'canada', 'brazil']:\n",
    "        if country in query_lower:\n",
    "            target_country = country\n",
    "            break\n",
    "    \n",
    "    # Extract target city if specified\n",
    "    target_city = None\n",
    "    for city in [c['city'].lower() for c in city_data]:\n",
    "        if city in query_lower:\n",
    "            target_city = city\n",
    "            break\n",
    "    \n",
    "    # Prepare data containers\n",
    "    altitude_data = []\n",
    "    population_data = []\n",
    "    landmark_data = []\n",
    "    climate_data = []\n",
    "    \n",
    "    # Collect all relevant city data\n",
    "    for city in city_data:\n",
    "        # Skip if country filter exists and doesn't match\n",
    "        if target_country and city['country'].lower() != target_country:\n",
    "            continue\n",
    "        \n",
    "        # Skip if city filter exists and doesn't match\n",
    "        if target_city and city['city'].lower() != target_city:\n",
    "            continue\n",
    "        \n",
    "        # Process altitude data\n",
    "        if city.get('altitude') and city['altitude'] != 'N/A':\n",
    "            try:\n",
    "                altitude = float(city['altitude'].replace('m', '')) if 'm' in city['altitude'] else float(city['altitude'])\n",
    "                altitude_data.append({\n",
    "                    'city': city['city'],\n",
    "                    'value': f\"{altitude}m\",\n",
    "                    'numeric': altitude\n",
    "                })\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Process population data\n",
    "        if city.get('population') and isinstance(city['population'], (int, float)):\n",
    "            population_data.append({\n",
    "                'city': city['city'],\n",
    "                'value': f\"{round(city['population']/1000000, 1)}M\",\n",
    "                'numeric': city['population']\n",
    "            })\n",
    "        \n",
    "        # Process landmark data\n",
    "        if city.get('landmarks') and city['landmarks'] not in ['N/A', '']:\n",
    "            landmark_data.append({\n",
    "                'city': city['city'],\n",
    "                'value': city['landmarks'].split('.')[0].strip()\n",
    "            })\n",
    "        \n",
    "        # Process climate data\n",
    "        if city.get('climate') and city['climate'] not in ['N/A', '']:\n",
    "            climate_data.append({\n",
    "                'city': city['city'],\n",
    "                'value': city['climate'].split('.')[0].strip()\n",
    "            })\n",
    "\n",
    "    # Determine question type and format response\n",
    "    # Altitude questions\n",
    "    if any(keyword in query_lower for keyword in ['altitude', 'elevation', 'height']):\n",
    "        if altitude_data:\n",
    "            altitude_data.sort(key=lambda x: x['numeric'], reverse='highest' in query_lower)\n",
    "            top_entry = altitude_data[0]\n",
    "            \n",
    "            if 'highest' in query_lower:\n",
    "                final_answer = f\"{top_entry['city']} has the highest altitude in {target_country.title() if target_country else 'the world'} at {top_entry['value']}.\"\n",
    "            elif 'lowest' in query_lower:\n",
    "                final_answer = f\"{top_entry['city']} has the lowest altitude in {target_country.title() if target_country else 'the world'} at {top_entry['value']}.\"\n",
    "            else:\n",
    "                final_answer = f\"{top_entry['city']} has altitude {top_entry['value']}.\"\n",
    "            \n",
    "            sub_responses = [f\"{x['city']}: {x['value']}\" for x in altitude_data[:3]]\n",
    "    \n",
    "    # Population questions\n",
    "    elif any(keyword in query_lower for keyword in ['population', 'populous']):\n",
    "        if population_data:\n",
    "            population_data.sort(key=lambda x: x['numeric'], reverse=True)\n",
    "            top_entry = population_data[0]\n",
    "            \n",
    "            if 'highest' in query_lower:\n",
    "                final_answer = f\"{top_entry['city']} has the highest population in {target_country.title() if target_country else 'the world'} at {top_entry['value']}.\"\n",
    "            elif 'lowest' in query_lower:\n",
    "                final_answer = f\"{top_entry['city']} has the lowest population in {target_country.title() if target_country else 'the world'} at {top_entry['value']}.\"\n",
    "            else:\n",
    "                final_answer = f\"{top_entry['city']} has population {top_entry['value']}.\"\n",
    "            \n",
    "            sub_responses = [f\"{x['city']}: {x['value']}\" for x in population_data[:3]]\n",
    "    \n",
    "    # Landmark questions\n",
    "    elif any(keyword in query_lower for keyword in ['landmark', 'place', 'attraction', 'tourist', 'visit', 'see']):\n",
    "        if landmark_data:\n",
    "            if target_city:\n",
    "                landmarks = [x['value'] for x in landmark_data if x['city'].lower() == target_city]\n",
    "                final_answer = f\"Places to visit in {target_city.title()}: {', '.join(landmarks[:3])}.\"\n",
    "            else:\n",
    "                landmarks = [f\"{x['city']}: {x['value']}\" for x in landmark_data]\n",
    "                final_answer = f\"Top landmarks: {', '.join(landmarks[:3])}.\"\n",
    "            \n",
    "            sub_responses = [f\"{x['city']}: {x['value']}\" for x in landmark_data[:3]]\n",
    "    \n",
    "    # Climate questions\n",
    "    elif any(keyword in query_lower for keyword in ['climate', 'weather', 'temperature']):\n",
    "        if climate_data:\n",
    "            if target_city:\n",
    "                climates = [x['value'] for x in climate_data if x['city'].lower() == target_city]\n",
    "                final_answer = f\"Climate in {target_city.title()}: {', '.join(climates[:3])}.\"\n",
    "            else:\n",
    "                climates = [f\"{x['city']}: {x['value']}\" for x in climate_data]\n",
    "                final_answer = f\"Climate information: {', '.join(climates[:3])}.\"\n",
    "            \n",
    "            sub_responses = [f\"{x['city']}: {x['value']}\" for x in climate_data[:3]]\n",
    "    \n",
    "    # Format the output\n",
    "    response_lines = [\"○ Sub-question responses:\"]\n",
    "    for i, resp in enumerate(sub_responses[:3], 1):\n",
    "        response_lines.append(f'■ \"{resp}\"')\n",
    "    response_lines.append(f'○ Final Response: \"{final_answer}\"')\n",
    "    \n",
    "    return '\\n'.join(response_lines)\n",
    "\n",
    "def answer_city_question(query: str) -> str:\n",
    "    \"\"\"Complete RAG pipeline execution\"\"\"\n",
    "    # Task 1: Generate sub-questions\n",
    "    sub_questions = generate_sub_questions(query)\n",
    "    \n",
    "    # Task 2: Retrieve answers for each sub-question\n",
    "    sub_answers = []\n",
    "    for sub_q in sub_questions:\n",
    "        cities = retrieve_city_data(sub_q)\n",
    "        answers = []\n",
    "        for city in cities:\n",
    "            # Try all requested attributes until we get a valid answer\n",
    "            for attr in sub_q[\"attributes\"]:\n",
    "                ans = format_sub_answer(city, attr)\n",
    "                if ans:\n",
    "                    answers.append(ans)\n",
    "                    break\n",
    "        \n",
    "        sub_answers.append({\n",
    "            \"question\": sub_q[\"question\"],\n",
    "            \"answers\": answers,\n",
    "            \"source\": sub_q.get(\"source\", \"city_dataset\")\n",
    "        })\n",
    "    \n",
    "    # Task 3: Generate final response\n",
    "    return generate_final_response(query, sub_answers)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_queries = [\n",
    "        \"Which city has the highest population in India?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nQuestion: {query}\")\n",
    "        print(answer_city_question(query))\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6301c84-d61d-46db-92e0-baaf04e6d97b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
